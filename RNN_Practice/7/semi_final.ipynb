{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Dense, Concatenate, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import keras_tuner as kt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv('NVspinData_None_-1_230807.csv')\n",
    "\n",
    "# 데이터셋 분리: train_set 8 : test_set 2\n",
    "train_df, test_df = train_test_split(df, shuffle=True, test_size=0.2)\n",
    "# train_set을 다시 8:2로 나눠서 train_set과 validation_set을 만듦\n",
    "train_df, val_df = train_test_split(train_df, shuffle=True, test_size=0.2)\n",
    "# random_state=42\n",
    "\n",
    "# 모든 시퀀스의 길이 중에서 최대 길이를 구하기\n",
    "all_sequences = [eval(seq) for seq in df['combination'].values]\n",
    "max_seq_length = max([len(seq) for seq in all_sequences])\n",
    "\n",
    "# 각 데이터셋에서 theta, phi, sequence 추출하고 reshape 적용\n",
    "theta_train = train_df['Theta'].values.reshape(-1, 1)\n",
    "phi_train = train_df['Phi'].values.reshape(-1, 1)\n",
    "sequence_train = pad_sequences(train_df['combination'].apply(eval).tolist(), maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "theta_val = val_df['Theta'].values.reshape(-1, 1)\n",
    "phi_val = val_df['Phi'].values.reshape(-1, 1)\n",
    "sequence_val = pad_sequences(val_df['combination'].apply(eval).tolist(), maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "theta_test = test_df['Theta'].values.reshape(-1, 1)\n",
    "phi_test = test_df['Phi'].values.reshape(-1, 1)\n",
    "sequence_test = pad_sequences(test_df['combination'].apply(eval).tolist(), maxlen=max_seq_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from my_dir\\keras_tuner_bayesian_rnn\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "78/78 [==============================] - 1s 4ms/step - loss: 0.2669 - accuracy: 0.9000\n",
      "Test Loss: 0.2669\n",
      "Test Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    theta_input = Input(shape=(1,), name='theta_input')\n",
    "    phi_input = Input(shape=(1,), name='phi_input')\n",
    "    merged = Concatenate()([theta_input, phi_input])\n",
    "    \n",
    "    repeated_vector = RepeatVector(max_seq_length)(merged)\n",
    "    \n",
    "    # RNN 레이어 동적 추가\n",
    "    rnn_output = repeated_vector\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):  # 1-3개의 RNN 레이어\n",
    "        rnn_output = SimpleRNN(units=hp.Int('rnn_units', min_value=16, max_value=128, step=16), \n",
    "                               return_sequences=True)(rnn_output)\n",
    "        \n",
    "    output = TimeDistributed(Dense(5, activation='softmax'), name='output_layer')(rnn_output)\n",
    "    \n",
    "    model = Model(inputs=[theta_input, phi_input], outputs=output)\n",
    "    \n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 베이지안 최적화 사용\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='my_dir',\n",
    "    project_name='keras_tuner_bayesian_rnn'\n",
    ")\n",
    "\n",
    "# 튜닝 시작\n",
    "tuner.search([theta_train, phi_train], np.expand_dims(sequence_train, -1),\n",
    "             validation_data=([theta_val, phi_val], np.expand_dims(sequence_val, -1)),\n",
    "             epochs=10, batch_size=64)\n",
    "\n",
    "# 최상의 모델 얻기\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# 검증\n",
    "loss, accuracy = best_model.evaluate([theta_test, phi_test], np.expand_dims(sequence_test, -1))\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is 128.\n",
      "The optimal optimizer is adam.\n",
      "The optimal learning rate for the optimizer is 0.006898389145724495.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is {best_hps.get('rnn_units')}.\n",
    "The optimal optimizer is {best_hps.get('optimizer')}.\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
